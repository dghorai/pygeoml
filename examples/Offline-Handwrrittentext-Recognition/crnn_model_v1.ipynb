{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Feb 27 18:08:54 2022\n",
    "\n",
    "@author: Debabrata Ghorai\n",
    "\n",
    "Objective: Handwritten Text Recognition using Tensorflow, Keras, and IAM Dataset\n",
    "\n",
    "Dataset Used: https://fki.tic.heia-fr.ch/databases/iam-handwriting-database\n",
    "\n",
    "References:\n",
    "1) U. Marti and H. Bunke. The IAM-database: An English Sentence Database for Off-line Handwriting Recognition. Int. Journal on Document Analysis and Recognition, Volume 5, pages 39 - 46, 2002.\n",
    "2) https://github.com/sudoaditya/Handwritten-Text-Recognition\n",
    "3) https://wandb.ai/authors/text-recognition-crnn-ctc/reports/Text-Recognition-With-CRNN-CTC-Network--VmlldzoxNTI5NDI\n",
    "4) https://keras.io/examples/vision/handwriting_recognition/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings in the output\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCLayer(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, name=None):\n",
    "        \n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = keras.backend.ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        # Compute the training-time loss value and add it to the layer using self.add_loss()\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        # At test time, just return the computed predictions\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(root):\n",
    "    images = list()\n",
    "    labels = list()\n",
    "    wordslist = list()\n",
    "    \n",
    "    # get the image dir and metadata path\n",
    "    imagepath = os.path.join(root, 'words')\n",
    "    metadata = os.path.join(root, 'words.txt')\n",
    "    \n",
    "    # defining object words to open metadata file in read mode\n",
    "    words = open(metadata, \"r\")\n",
    "    # reading each line from original text file\n",
    "    for line in words.readlines():\n",
    "        # reading all lines that do not begin with \"#\"\n",
    "        if not (line.startswith('#')):\n",
    "            # splitting line and search correctly word to append into list\n",
    "            if line.split(\" \")[1] == \"ok\":\n",
    "                wordslist.append(line)\n",
    "                \n",
    "    # shuffle the elements in list\n",
    "    np.random.shuffle(wordslist)\n",
    "    # loop over list of words\n",
    "    for i, line in enumerate(wordslist):\n",
    "        line = line.strip()\n",
    "        line = line.split(\" \")\n",
    "        # get image path\n",
    "        img_name = line[0]\n",
    "        img_path = os.path.join(imagepath, img_name.split(\"-\")[0], img_name.split(\"-\")[0] + \"-\" + img_name.split(\"-\")[1], img_name + \".png\")\n",
    "        if os.path.exists(img_path):\n",
    "            images.append(img_path)\n",
    "            labels.append(line[-1].strip())\n",
    "            \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_process(image, width=None, height=None):\n",
    "    # final image\n",
    "    dim = (width, height)\n",
    "    #X = cv2.imread(img, cv2.IMREAD_UNCHANGED)\n",
    "    X = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
    "    # print('Original Dimensions : ', X.shape)\n",
    "    try:\n",
    "        X = cv2.resize(X, dim, interpolation=cv2.INTER_AREA)\n",
    "    except:\n",
    "        X = cv2.resize(X.astype('uint8'), dim, interpolation=cv2.INTER_AREA)\n",
    "    # scale\n",
    "    X = X / 255.0\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(label_data, islist=True):\n",
    "    characters = set()\n",
    "    max_word_len = 0\n",
    "    # loop over label data\n",
    "    for label in label_data:\n",
    "        for char in label:\n",
    "            characters.add(char)\n",
    "        # get max len\n",
    "        max_word_len = max(max_word_len, len(label))\n",
    "    # unique characters\n",
    "    if islist == False:\n",
    "        characters = \"\". join([str(s) for s in characters])\n",
    "    return characters, max_word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(images, labels, char_to_num, max_word_len, width=None, height=None):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    for image, label in zip(images, labels):\n",
    "        try:\n",
    "            ix = image_process(image, width=width, height=height)\n",
    "            ix = tf.convert_to_tensor(ix, dtype=tf.float32)\n",
    "            ix = tf.transpose(ix, perm=[1,0,2])\n",
    "            iy = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
    "            iy = tf.pad(iy, paddings=[[0, max_word_len - iy.shape[0]]], constant_values=999)\n",
    "            x_data.append(ix)\n",
    "            y_data.append(iy)\n",
    "        except:\n",
    "            pass\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(width, height, characters):\n",
    "    # Model Inputs\n",
    "    images = keras.layers.Input(shape=(width, height, 1), name=\"image\", dtype=\"float32\")\n",
    "    labels = keras.layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n",
    "    # Create CNN Layers\n",
    "    x = keras.layers.Conv2D(32,(3, 3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\",name=\"Conv1\",)(images)\n",
    "    x = keras.layers.Conv2D(64,(3, 3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\",name=\"Conv2\",)(x)\n",
    "    x = keras.layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n",
    "    x = keras.layers.Conv2D(128,(3, 3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\",name=\"Conv3\",)(x)\n",
    "    x = keras.layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n",
    "    # Reshape the Layer before passing the output to RNN layer\n",
    "    x = keras.layers.Reshape(target_shape=(x.shape[1], x.shape[2]*x.shape[3]), name=\"reshape\")(x)\n",
    "    x = keras.layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    # Create RNN Layers\n",
    "    x = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n",
    "    x = keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n",
    "    # Output layer\n",
    "    x = keras.layers.Dense(len(characters)+1 , activation=\"softmax\", name=\"dense2\")(x)\n",
    "    # Add CTC Layer to calculate ctc loss at each step\n",
    "    output = CTCLayer(name='ctc_loss')(labels, x)\n",
    "    # Define the model\n",
    "    model = keras.models.Model(inputs=[images, labels], outputs=output, name=\"ocr_model\")\n",
    "    # Compile the model and return\n",
    "    model.compile(optimizer=Adam(learning_rate = 0.0001))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_batch_predictions(pred, max_len, num_to_char):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search\n",
    "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][:, :max_len]\n",
    "    # Iterate over the results and get back the text\n",
    "    output_text = []\n",
    "    for res in results:\n",
    "        res = tf.gather(res, tf.where(tf.math.not_equal(res, -1)))\n",
    "        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(res)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(images, labels, width, height, characters, char_to_num, max_word_len):\n",
    "    # train-test split of the model inputs\n",
    "    x_data, y_data = prepare_data(images, labels, char_to_num, max_word_len, width=width, height=height)\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_data, y_data, test_size=0.2, random_state=1)\n",
    "    \n",
    "    # convert list of numpy array to tensor\n",
    "    x_train = tf.stack(x_train)\n",
    "    x_valid = tf.stack(x_valid)\n",
    "    y_train = tf.stack(y_train)\n",
    "    y_valid = tf.stack(y_valid)\n",
    "    \n",
    "    # get the model (considering maximum time step 32)\n",
    "    model = build_model(width, height, characters)\n",
    "    model.summary()\n",
    "    \n",
    "    # set the callbacks\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath=\"best_model.h5\", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='min')\n",
    "    earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "    callbacks_list = [checkpoint, earlystop]\n",
    "    \n",
    "    # train the model\n",
    "    history = model.fit(\n",
    "        [x_train, y_train], \n",
    "        validation_data=[x_valid, y_valid], \n",
    "        epochs=100, \n",
    "        batch_size=64, \n",
    "        callbacks=callbacks_list, \n",
    "        verbose=1,\n",
    "        )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(view_model_performance=True, view_pred_output=True):\n",
    "    width = 128\n",
    "    height = 32\n",
    "    # get image paths and labels\n",
    "    images_tot, labels_tot = get_data(root_dir)\n",
    "    # get total characters and maximum word length\n",
    "    characters, max_word_len = get_vocabulary(labels_tot, islist=True)\n",
    "    \n",
    "    # convert character to number for model training\n",
    "    char_to_num = keras.layers.experimental.preprocessing.StringLookup(\n",
    "        vocabulary=list(characters),\n",
    "        num_oov_indices=0,\n",
    "        mask_token=None\n",
    "        )\n",
    "    \n",
    "    # convert number to character for decode prediction\n",
    "    num_to_char = keras.layers.experimental.preprocessing.StringLookup(\n",
    "        vocabulary=char_to_num.get_vocabulary(),\n",
    "        mask_token=None,\n",
    "        invert=True\n",
    "        )\n",
    "    \n",
    "    # train-test split\n",
    "    images_train, images_test, labels_train, labels_test = train_test_split(images_tot, labels_tot, test_size=0.5, random_state=1)\n",
    "    \n",
    "    # training model\n",
    "    history = model_training(images_train, labels_train, width, height, characters, char_to_num, max_word_len)\n",
    "    \n",
    "    if view_model_performance == True:\n",
    "        # view model performance\n",
    "        loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "        epochs = range(1,len(loss)+1)\n",
    "        plt.plot(epochs, loss, 'b')\n",
    "        plt.plot(epochs, val_loss, 'r')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Val'], loc='upper left')\n",
    "        plt.show()\n",
    "    \n",
    "    if view_pred_output == True:\n",
    "        # visualize prediction (accuracy)\n",
    "        xdata, ydata = prepare_data(images_test, labels_test, char_to_num, max_word_len, width=width, height=height)\n",
    "        xdata, ydata = tf.stack(xdata), tf.stack(ydata)\n",
    "        # model prediction\n",
    "        model = build_model(width, height, characters)\n",
    "        prediction_model = keras.models.Model(model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output)\n",
    "        prediction_model.load_weights(\"best_model.h5\")\n",
    "        preds = prediction_model.predict(xdata)\n",
    "        out_text = decode_batch_predictions(preds, max_word_len, num_to_char)\n",
    "        # see the results\n",
    "        for i, x in enumerate(out_text[0:10]):\n",
    "            print(\"original_text =  \", labels_test[i])\n",
    "            print(\"predicted text = \", x)\n",
    "            plt.imshow(images_test[i].reshape(height,width), cmap=plt.cm.gray)\n",
    "            plt.show()\n",
    "            print('\\n')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    root_dir = r\"\\...\\Practical\\IAM_Handwriting_Database\"\n",
    "    os.chdir(root_dir) # save best model in this root_dir only\n",
    "    main(view_model_performance=False, view_pred_output=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
